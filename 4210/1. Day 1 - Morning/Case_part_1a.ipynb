{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case: Model Operationalization\n",
    "### Part 1a: Training a model in the development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first part of the case! In this step we will train the model which we will later deploy into our production environment. \n",
    "The goal of this step is to get a feeling for the data and the model that were dealing with. \n",
    "As in many data science departments, you will use Jupyter notebook to train the model. \n",
    "Using Jupyter Notebook enables us to make our script readable and easily explainable to others, because of the options to include visualizations and text blocks.\n",
    "\n",
    "Run the code and have a quick look to understand what is going on. The whole notebook should run without problem already. Check out the code and try to figure out which pickle files you have to make. Don't spend too much time on  understanding the details of the code!\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# We set a random seed to get the same results with every run\n",
    "random.seed(15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains HR data - for each employee we have collected data and we mark if he had left the company or not.\n",
    "We would like to train a model to alert us on potential employees who might leave us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the csv file in the same directory as this notebook or update the path below\n",
    "csv_path = \"HR_source_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata = pd.read_csv(r\"{}\".format(csv_path), header=0, sep=\",\")\n",
    "inputdata.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's review some statistics\n",
    "\n",
    "inputdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use specific variables for the model\n",
    "\n",
    "columns = ['average_monthly_hours', 'department', 'salary', 'number_project', 'last_evaluation', 'satisfaction_level', 'left']\n",
    "inputdata = inputdata.loc[:, columns]\n",
    "inputdata.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does department affects churn? And what about other columns?\n",
    "# Plot categories histogram and split with explained variable\n",
    "g=sns.catplot(x=\"department\", hue=\"left\", kind=\"count\",\n",
    "            palette=\"pastel\", edgecolor=\".6\",\n",
    "            data=inputdata)\n",
    "g.fig.set_size_inches(12, 12)\n",
    "\n",
    "# And as a table - average churn rate per department\n",
    "print(inputdata.groupby('department').mean()['left'])\n",
    "\n",
    "# Churn is lower in RandD and managment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to build a model, the following transformations on the data will have to be applied:\n",
    "1. Dummify categorical variables\n",
    "2. Create a new variable \n",
    "3. Scale and Impute certain variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and test\n",
    "train_img, test_img = train_test_split(inputdata, test_size=0.3, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter department variables based on exploration\n",
    "train_img['department'] = train_img['department'].apply(lambda x: 'other' if x not in ['RandD','management'] else x)\n",
    "\n",
    "# Dummify the department variable  \n",
    "train_img = pd.concat([train_img, pd.get_dummies(train_img['department'])], axis=1)\n",
    "train_img.drop('department', axis=1, inplace=True)\n",
    "train_img.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make salary an ordinal variable\n",
    "\n",
    "replace_map_salary = {'salary': {'low': 1, 'medium': 2, 'high': 3}}\n",
    "train_img.replace(replace_map_salary, inplace=True)\n",
    "train_img.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable - number of hours per project\n",
    "\n",
    "train_img['hours_per_project'] = train_img['average_monthly_hours']/train_img['number_project']\n",
    "train_img.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the average monthly hours variable using the min-max scaler\n",
    "\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "train_img['average_monthly_hours'] = scaler.fit_transform(train_img['average_monthly_hours'].values.reshape(-1, 1))\n",
    "train_img.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dealing with missing values\n",
    "# Impute satisfaction level using the mean of the variable\n",
    "\n",
    "train_img.satisfaction_level.fillna(train_img.satisfaction_level.mean(), inplace=True)\n",
    "train_img.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Random Forest for classficiation\n",
    "# More hyperparameters can be tweaked, but we keep it simple for now --> not the focus of this module\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_img.drop(['left'], axis=1)\n",
    "y_train = train_img.loc[:, 'left']\n",
    "\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using feature importance, we can better understand which variables are contributing to the model the most\n",
    "\n",
    "features = X_train.columns\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to transform the test set to the version we can use for model training.\n",
    "# In other words, apply the same transformations as we have done during our exploration phase, but now in one function.\n",
    "def test_transformation(test_set):\n",
    "    # Create a local copy of the incoming data in this function.\n",
    "    test_set_copy = test_set.copy()\n",
    "    \n",
    "    # Transform department variable\n",
    "    test_set_copy['department'] = test_set_copy['department'].apply(lambda x: 'other' if x not in ['RandD', 'management'] else x)\n",
    "    \n",
    "    # Dummify categorical variable \n",
    "    test_set_copy = pd.concat([test_set_copy, pd.get_dummies(test_set_copy['department'])], axis=1)\n",
    "    \n",
    "    # Remove old categorical variable \n",
    "    test_set_copy.drop('department', axis=1, inplace=True)\n",
    "    \n",
    "    # Make salary variable ordinal\n",
    "    test_set_copy.replace(replace_map_salary, inplace=True)\n",
    "    \n",
    "    # Create new variable\n",
    "    test_set_copy['hours_per_project'] = test_set_copy['average_monthly_hours'] / test_set_copy['number_project']\n",
    "    \n",
    "    # Scale  average monthly hours\n",
    "    test_set_copy['average_monthly_hours'] = scaler.transform(test_set_copy['average_monthly_hours'].values.reshape(-1,1))\n",
    "    \n",
    "    # Impute missing values in satisfaction level\n",
    "    test_set_copy.satisfaction_level.fillna(train_img.satisfaction_level.mean(), inplace=True)\n",
    "    \n",
    "    return test_set_copy.drop(['left'], axis=1),test_set_copy.loc[:, 'left']\n",
    "\n",
    "X_test, y_test = test_transformation(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expected = y_test\n",
    "\n",
    "# Make new predictions\n",
    "predicted = forest.predict(X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "print(classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: save relevant model objects to be able to deploy the model to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip - Go through the theory to recall which model objects are relevant to save.\n",
    "# Use the pickle package documentation to save the model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
